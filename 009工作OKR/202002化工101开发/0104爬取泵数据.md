# 泵数据

[真空泵型号,真空泵生产厂家,真空泵系列-浙江扬子江泵业有限公司](http://www.yzj.cc/pro-list.asp?ppid=46)

[排污泵型号,排污泵生产厂家,排污泵系列-浙江扬子江泵业有限公司](http://www.yzj.cc/pro-list.asp?ppid=44)

## 01. requests + bs4 抓取

### 1. 过程记录

```py
import requests
from bs4 import BeautifulSoup

url = 'http://www.yzj.cc/pro-list.asp?ppid=44'
re = requests.get(url)
soup = BeautifulSoup(re.content, 'html.parser')

```

### 2. 遇到的问题

1、BS4 里如何抓取锚元素 \<a> 里的超链接。

之前是知道如何获取常规元素里的值，比如：

    soup.findAll('div', class_="big_li")
    
但 div 下面的 \<a> 里面的 href 就不知道了，因为它不是 text 文本。查得：[How can I get href links from HTML using Python? - Stack Overflow](https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python)，里面的信息：

```py
from BeautifulSoup import BeautifulSoup
import urllib2
import re

html_page = urllib2.urlopen("http://www.yourwebsite.com")
soup = BeautifulSoup(html_page)
for link in soup.findAll('a'):
    print link.get('href')
```

受到启发：

```py
for link in soup.findAll('div', class_="big_li")[0].findAll('a'):
    print(link.get('href'))
```

改进：

```py
# 因为得到的是个单元素列表，所以取第一个元素
[link.get('href') for link in soup.findAll('div', class_="big_li")[0].findAll('a')][0]
```

获取页面里所有设备分类超链接的话：

```
for i in soup.findAll('div', class_="big_li"):
    print([link.get('href') for link in i][0])
```

获取某个分类页的 URL：

    'http://www.yzj.cc/'+[link.get('href') for link in soup.findAll('div', class_="big_li")[0].findAll('a')][0]

获取简介信息：

    temp = soup1.findAll('div', class_="jianjie")

发现无法通过 get_text() 获取元素里面的内容，通过 temp? 获取有关 temp 的信息，Docstring:  A ResultSet is just a list that keeps track of the SoupStrainer，又是一个 SoupStrainer，尝试跟获取 href 采用同样的办法：

    [i.get_text() for i in temp]

不过得清洗，比如：

```
summary = [i.get_text().replace('\n', '') for i in temp]
summary = summary[0].replace('\t', '')
summary = summary.replace('\r', '')
summary = summary.replace(' ', '')
```

对比下来在 scrapy 里用 xpath 提取信息简单多了。

2『原生态系统能否也可以用 xpath 提取，待探索确认。』

## 02. scrapy + request + bs4

### 1. 引入自定义模块

在 seetting 里看到这么一个信息：

    NEWSPIDER_MODULE = 'equipment.spiders'
    
猜测这是自定义模块的路径，所以在 spiders 文件夹里新建文件「utilmodule.py」，然后在 basic 里引入：

    import equipment.spiders.utilmodule as util

### 2. 表格数据的抓取

```py
response.xpath('(//td/div/text()) | (//td/div/div/text())').extract()
response.xpath('//td/p/text()').extract()
```

[关于转义字符 \\t \\r \\n_移动开发_duanxj的博客-CSDN博客](https://blog.csdn.net/duanxj00001/article/details/54315185)

\t \r \n 都是转义字符，空格就是单纯的空格，输入时可以输入空格。

\t 的意思是横向跳到下一制表符位置。

\r 的意思是回车。

\n 的意思是回车换行。

### 3. url 数据

最底层的详细页面如果找不到当页的 url 的话，可以在其上一级页面抓取，但要额外再增加一个属性字段作为跟大数据表关联合并时用，比如泵大类系列产品的 title，详见「04-20」的代码。

其中注意一个细节，yield item 一定要放在那个循环里，否者只能抓取 parse() 方法（上一级页面）里一个 URL 的数据。具体底层的逻辑目前弄不清楚，mark 下。（2020-04-20）

jupyter 里清洗记录：

```
# json 转成 dataframe 格式
df = pd.read_json("data.json",encoding="utf-8", orient='records')

# 与原有数据合并
df = pd.merge(df1, df2, on='title', how='outer')
df = df.drop_duplicates(['title'])

# 删除型号字段「devicetype」为空的数据
df1 = df.dropna(subset=['devicetype'])
```

注：

获取某列里某行是否为空的布尔值：

```
df['devicetype'].isna()
~df['devicetype'].isna()
```

更简洁的删除方法是前面的：

```
df1 = df.dropna(subset=['devicetype'])
```

### 遇到的问题

1、简洁的信息「briefinfo」不能再在 scrapy 里清洗，估计是抓取的时候有些页面不是列表数据，不能统一用列表的方式来处理。还是放到后端用 pandas 清洗。

## 03. pandas 数据清洗

### 1. 正则匹配清洗

[正则匹配HTML标签（div，p等）适用于replace_JavaScript_小不点的博客-CSDN博客](https://blog.csdn.net/zhang__ao/article/details/79214395)

```py
 m = re.search('<img(([\s\S])*?)>', test)
<div(([\s\S])*?)<\/div>
```

这是后端匹配的思路，发现可以在抓取的时候前端匹配好。

```py
intro = ''.join(response.xpath('(//div[@class="showpr_msg"]//h3/text()) | (//div[@class="showpr_msg"]//p/text()) | (//div[@class="showpr_msg"]//a/text()) | (//div[@class="showpr_msg"]//span/text())').extract())
intro = re.sub('[\t\r\xa0]', '', intro)
re.sub(' ', '', intro)
```

### 2. 清洗数据

```py
import re

# json 数据转为 dataframe
df = pd.read_json("pumpdata.json",encoding="utf-8", orient='records')

# 用这个清洗
df.loc[:,'briefinfo'] = pump_df['briefinfo'].apply(lambda x: re.sub('[\r\n\t]', '',x))

# 以 title 字段为准删除重复的
df = pump_df.drop_duplicates(['title'])

# 去掉头部的换行符
df.loc[:,'briefinfo'] = df['briefinfo'].apply(lambda x: re.sub('^[\n]+', '',x))
df.loc[:,'intro'] = df['intro'].apply(lambda x: re.sub('^[\n]+', '',x))

# 增加一个大类字符
df.loc[:,'class'] = 'pump'

# 列表连接成字符串，否则无法存入 mysql
df.loc[:,'devicetype'] = df['devicetype'].apply(lambda x: ''.join(x))

# 传数据库
from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://root@localhost:3306/shop')
df.to_sql('tz_pump', engine, index= False)

df = df.loc[:,['title', 'briefinfo', 'bigclass', 'intro', 'class']]
```

拆解字段：

```
# 概述
df['summary'] = df['intro'].astype('str').apply(lambda x: re.search(r'((概述\n\n)(.*\n\n)*).*意义', x).group(1) if re.search(r'(概述\n\n)(.*\n\n)*.*意义', x) else '')

# 安装与使用
df['install'] = df['intro'].astype('str').apply(lambda x: re.search(r'(安装与?使用\n\n)(.*\n\n)*.*注意事项', x).group() if re.search(r'(?s)(安装与?使用\n\n).*注意事项', x) else '')

# 注意事项
df['matters'] = df['intro'].astype('str').apply(lambda x: re.search(r'(注意事项\n\n)(.*\n\n)*', x).group() if re.search(r'(注意事项\n\n)(.*\n\n)*', x) else '')
```

获取大类的数据：

```
pump_df.drop_duplicates(['bigclass'])['bigclass'].values
['计量泵系列', '管道泵系列', '油泵系列', '消防泵系列', '真空泵系列', '液下泵系列', '配套阀门系列',
       '配套系列', '污水泵系列', '不锈钢耐腐蚀泵', '水力喷射器系列', '耐腐蚀泵系列', '给排水设备系列',
       '潜水泵系列', '漩涡泵系列', '多级泵系列', '螺杆泵系列', '化工泵系列', '隔膜泵系列', '离心泵系列',
       '排污泵系列', '自吸泵系列', '磁力泵系列']
```

### 属性字段记录

briefinfo，原始简介。

bigclass，设备大类系列。

title，泵系列名称。

intro，原始设备信息，需要拆分。

devicetype，设备型号。

breakdown，设备其他表格。

class，设备所属的总大类，比如输送泵 pump。

summary，概述。

install，安装与使用。

matters，注意事项。

coverintro，封面介绍。


## 代码版本

2020-03-28

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):
        # raw_summary = response.xpath('//div[@class="jianjie"]/text()').extract()[0].strip()
        # item['summary'] = re.sub('[\r\t\n]', '', raw_summary)

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_detail_pages)
        
        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_detail_pages(self, response):
        item = EquipmentItem()

        # raw_summarys = response.xpath('//div[@class="prjianjie"]/p/text()').extract()
        # summarys = []
        # for item in raw_summarys:
        #     item = item.strip()
        #     summarys.append(re.sub('[\r\t\n]', '', item))
        # item['briefinfo'] = ''.join(summarys)
        item['briefinfo'] = ''.join(response.xpath('//div[@class="prjianjie"]/p/text()').extract())

        item['bigclass'] = response.xpath('//div[@class="wz_title"]/text()').extract()[0]
        item['title'] = response.xpath('//h1/text()').extract()[0]

        yield item
        
```

2020-03-30

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_detail_pages)
        
        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_detail_pages(self, response):
        item = EquipmentItem()

        briefinfo = ''.join(response.xpath('//div[@class="prjianjie"]/p/text()').extract())
        item['briefinfo'] = re.sub('[\r\t]', '',briefinfo)

        item['bigclass'] = response.xpath('//div[@class="wz_title"]/text()').extract()[0]
        item['title'] = response.xpath('//h1/text()').extract()[0]

        intro = ''.join(response.xpath('(//div[@class="showpr_msg"]//h3/text()) | (//div[@class="showpr_msg"]//p/text()) | (//div[@class="showpr_msg"]//a/text()) | (//div[@class="showpr_msg"]//span/text())').extract())
        intro = re.sub('[\t\r\xa0]', '', intro)
        item['intro'] = re.sub(' ', '', intro)

        yield item
        
```

2020-03-31（全部数据，包括表格里的型号）

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_detail_pages)
        
        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_detail_pages(self, response):
        item = EquipmentItem()

        briefinfo = ''.join(response.xpath('//div[@class="prjianjie"]/p/text()').extract())
        item['briefinfo'] = re.sub('[\r\t]', '',briefinfo)

        item['bigclass'] = response.xpath('//div[@class="wz_title"]/text()').extract()[0]
        item['title'] = response.xpath('//h1/text()').extract()[0]

        intro = ''.join(response.xpath('(//div[@class="showpr_msg"]//h3/text()) | (//div[@class="showpr_msg"]//p/text()) | (//div[@class="showpr_msg"]//a/text()) | (//div[@class="showpr_msg"]//span/text())').extract())
        intro = re.sub('[\t\r\xa0]', '', intro)
        item['intro'] = re.sub(' ', '', intro)

        item['devicetype'] = response.xpath('(//td/div/text()) | (//td/div/div/text())').extract()

        item['breakdown'] = response.xpath('//td/p/text()').extract()

        yield item
        
```

2020-04-20（URL 数据）

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):
        item = EquipmentItem()

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]
        titles = response.xpath('//a[@class="jzimg fl"]/img/@alt').extract()
        for i,j in dict(zip(titles, urls)).items():
            item['title'] = i
            item['url'] = j
            # yield 语句必须放在这个循环里，否者只能抓取 parse() 方法里一个 URL 数据
            yield item

        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)
        
```

