# 0701泵

[真空泵型号,真空泵生产厂家,真空泵系列-浙江扬子江泵业有限公司](http://www.yzj.cc/pro-list.asp?ppid=46)

[排污泵型号,排污泵生产厂家,排污泵系列-浙江扬子江泵业有限公司](http://www.yzj.cc/pro-list.asp?ppid=44)

## 01. 抓取记录

### requests + bs4 

```py
import requests
from bs4 import BeautifulSoup

url = 'http://www.yzj.cc/pro-list.asp?ppid=44'
re = requests.get(url)
soup = BeautifulSoup(re.content, 'html.parser')

```

## 02. 过程中遇到的问题

### 01

如何抓取锚元素 \<a> 里的超链接。

之前是知道如何获取常规元素里的值，比如：

    soup.findAll('div', class_="big_li")
    
但 div 下面的 \<a> 里面的 href 就不知道了，因为它不是 text 文本。查得：[How can I get href links from HTML using Python? - Stack Overflow](https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python)，里面的信息：

```py
from BeautifulSoup import BeautifulSoup
import urllib2
import re

html_page = urllib2.urlopen("http://www.yourwebsite.com")
soup = BeautifulSoup(html_page)
for link in soup.findAll('a'):
    print link.get('href')
```

受到启发：

```py
for link in soup.findAll('div', class_="big_li")[0].findAll('a'):
    print(link.get('href'))
```

改进：

```py
# 因为得到的是个单元素列表，所以取第一个元素
[link.get('href') for link in soup.findAll('div', class_="big_li")[0].findAll('a')][0]
```

获取页面里所有设备分类超链接的话：

```
for i in soup.findAll('div', class_="big_li"):
    print([link.get('href') for link in i][0])
```

获取某个分类页的 URL：

    'http://www.yzj.cc/'+[link.get('href') for link in soup.findAll('div', class_="big_li")[0].findAll('a')][0]

获取简介信息：

    temp = soup1.findAll('div', class_="jianjie")

发现无法通过 get_text() 获取元素里面的内容，通过 temp? 获取有关 temp 的信息，Docstring:  A ResultSet is just a list that keeps track of the SoupStrainer，又是一个 SoupStrainer，尝试跟获取 href 采用同样的办法：

    [i.get_text() for i in temp]

不过得清洗，比如：

```
summary = [i.get_text().replace('\n', '') for i in temp]
summary = summary[0].replace('\t', '')
summary = summary.replace('\r', '')
summary = summary.replace(' ', '')
```

## scrapy + request + bs4

### 1. 引入自定义模块

在 seetting 里看到这么一个信息：

    NEWSPIDER_MODULE = 'equipment.spiders'
    
猜测这是自定义模块的路径，所以在 spiders 文件夹里新建文件「utilmodule.py」，然后在 basic 里引入：

    import equipment.spiders.utilmodule as util

### 2. 遇到的问题

1、简洁的信息「briefinfo」不能再在 scrapy 里清洗，估计是抓取的时候有些页面不是列表数据，不能统一用列表的方式来处理。还是放到后端用 pandas 清洗。


### 3. pandas 数据清洗

```py
import re

pump_df = pd.read_json("pumpdata.json",encoding="utf-8", orient='records')
pump_df.loc[:,'briefinfo'] = pump_df['briefinfo'].apply(lambda x: re.sub('[\r\n\t]', '',x))
```

正则匹配清洗：

[正则匹配HTML标签（div，p等）适用于replace_JavaScript_小不点的博客-CSDN博客](https://blog.csdn.net/zhang__ao/article/details/79214395)

```py
 m = re.search('<img(([\s\S])*?)>', test)

<div(([\s\S])*?)<\/div>

```

这是后端匹配的思路，发现可以在抓取的时候前端匹配好。

```py
intro = ''.join(response.xpath('(//div[@class="showpr_msg"]//h3/text()) | (//div[@class="showpr_msg"]//p/text()) | (//div[@class="showpr_msg"]//a/text()) | (//div[@class="showpr_msg"]//span/text())').extract())
intro = re.sub('[\t\r\xa0]', '', intro)
re.sub(' ', '', intro)
```


## 代码版本

2020-03-28

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):
        # raw_summary = response.xpath('//div[@class="jianjie"]/text()').extract()[0].strip()
        # item['summary'] = re.sub('[\r\t\n]', '', raw_summary)

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_detail_pages)
        
        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_detail_pages(self, response):
        item = EquipmentItem()

        # raw_summarys = response.xpath('//div[@class="prjianjie"]/p/text()').extract()
        # summarys = []
        # for item in raw_summarys:
        #     item = item.strip()
        #     summarys.append(re.sub('[\r\t\n]', '', item))
        # item['briefinfo'] = ''.join(summarys)
        item['briefinfo'] = ''.join(response.xpath('//div[@class="prjianjie"]/p/text()').extract())

        item['bigclass'] = response.xpath('//div[@class="wz_title"]/text()').extract()[0]
        item['title'] = response.xpath('//h1/text()').extract()[0]

        yield item
        
```

2020-03-30

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.yzj.cc/pro-list.asp?ppid=44']

    def parse(self, response):
        raw_urls = response.xpath('//div[@class="big_li"]/a/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):

        raw_urls = response.xpath('//a[@class="jzimg fl"]/@href').extract()
        urls = []
        [urls.append('http://www.yzj.cc/'+i) for i in raw_urls]
        # [print(i) for i in urls]

        for url in urls:
            yield response.follow(url, callback=self.parse_detail_pages)
        
        raw_nexturls = response.xpath('//div[@class="page"]/a/@href').extract()
        nexturls = []
        [nexturls.append('http://www.yzj.cc/pro-list.asp'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_detail_pages(self, response):
        item = EquipmentItem()

        briefinfo = ''.join(response.xpath('//div[@class="prjianjie"]/p/text()').extract())
        item['briefinfo'] = re.sub('[\r\t]', '',briefinfo)

        item['bigclass'] = response.xpath('//div[@class="wz_title"]/text()').extract()[0]
        item['title'] = response.xpath('//h1/text()').extract()[0]

        intro = ''.join(response.xpath('(//div[@class="showpr_msg"]//h3/text()) | (//div[@class="showpr_msg"]//p/text()) | (//div[@class="showpr_msg"]//a/text()) | (//div[@class="showpr_msg"]//span/text())').extract())
        intro = re.sub('[\t\r\xa0]', '', intro)
        item['intro'] = re.sub(' ', '', intro)

        yield item
        
```