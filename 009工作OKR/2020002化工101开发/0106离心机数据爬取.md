## 01. 数据源

赛德力：

[卧式离心机系列厂家|定制规格|采购选型|销售价格-江苏赛德力](http://www.saideli.com/productlist_cn/typeid/1.html)

博伦：

[离心机维修|离心机配件|卧螺离心机|三足式离心机|刮刀离心机价格-湘潭博伦离心机有限公司](http://www.bolun-centrifuge.com/index.html)

德高：

[张家港市德高机械制造有限公司__立式离心机系列、卧式离心机系列、LLW螺旋进料型、平板式/三足式人工卸料离心机](https://www.zjgdgjx.com/)

## 02. 爬取数据

### 1. 网页爬取

```py
# -*- coding: utf-8 -*-
import scrapy
import requests, re
from bs4 import BeautifulSoup

from equipment.items import EquipmentItem
import equipment.spiders.utilmodule as util

class BasicSpider(scrapy.Spider):
    name = 'basic'
    start_urls = ['http://www.saideli.com/productlist_cn/typeid/1.html']
    bigclass_dicts = {}

    def parse(self, response):
        raw_urls = response.xpath('//dd[@class="elps"]/a/@href').extract()
        urls = []
        [urls.append('http://www.saideli.com'+i) for i in raw_urls]
        bigclass = response.xpath('//dd[@class="elps"]/a/@title').extract()
        self.bigclass_dicts = dict(zip(urls, bigclass))
        for url in urls:
            yield response.follow(url, callback=self.parse_bigli_pages)

    def parse_bigli_pages(self, response):
        item = EquipmentItem()
        raw_urls = response.xpath('//ul[@class="cp-list clear"]//a/@href').extract()
        urls = []
        [urls.append('http://www.saideli.com'+i) for i in raw_urls]
        titles = response.xpath('//ul[@class="cp-list clear"]//a/@title').extract()
        for i,j in dict(zip(titles, urls)).items():
            item['title'] = i
            item['url'] = j
            # yield 语句必须放在这个循环里，否者只能抓取 parse() 方法里一个 URL 数据
            yield item

        raw_nexturls = response.xpath('//a[@class="next"]/@href').extract()
        nexturls = []
        [nexturls.append('http://www.saideli.com'+i) for i in raw_nexturls]
        for url in nexturls:
            yield response.follow(url, callback=self.parse_bigli_pages)
        
```

bigclass 字段目前没实现抓取，抓取到的有 title 和 URL 字段，难点在二级页面里没有 bigclass 字段，一级页面的 bigclass 不知道该如何传到二级页面去。

### 2. 清洗

```py
df = pd.read_json('data.json', encoding='utf-8', orient='records')
```

导出为 excel 添加相关信息。

格式规整：

```py
# 规整各个字段
df.loc[:,'title'] = df['title'].apply(lambda x: dalong.modify_text(x))

# 合并数据表
df = pd.concat([df, df1], ignore_index=True)

# 型号有具体参数的都转为 1，~df['devicetype'].isna() 是一个布尔值序列，可以传递进入
df['devicetype'][~df['devicetype'].isna()] = 1 

# 传数据库
from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://root@localhost:3306/shop')
df.to_sql('tz_pump', engine, index= False)
```

数据库里变更数据类型

```
ALTER TABLE `tz_pump` MODIFY COLUMN `title` VARCHAR(255) COMMENT '设备名称';
ALTER TABLE `tz_pump` MODIFY COLUMN `bigclass` VARCHAR(255) COMMENT '设备系列';
ALTER TABLE `tz_pump` MODIFY COLUMN `class` VARCHAR(255) COMMENT '设备大类';
ALTER TABLE `tz_pump` MODIFY COLUMN `url` VARCHAR(255) COMMENT '数据来源';
```

devicetype 字段改为 int，1 表示有具体的设备参数。

### 3. 设备参数

```
ALTER TABLE `tz_typesize` MODIFY COLUMN `bigclass` VARCHAR(255) COMMENT '设备系列';
ALTER TABLE `tz_typesize` MODIFY COLUMN `title` VARCHAR(255) COMMENT '设备名称';
ALTER TABLE `tz_typesize` MODIFY COLUMN `class` VARCHAR(255) COMMENT '设备大类';
ALTER TABLE `tz_typesize` MODIFY COLUMN `typeid` VARCHAR(255) COMMENT '设备型号';
ALTER TABLE `tz_typesize` MODIFY COLUMN `drum_diameter` VARCHAR(255) COMMENT '转鼓直径';
ALTER TABLE `tz_typesize` MODIFY COLUMN `drum_height` VARCHAR(255) COMMENT '转鼓高度';
ALTER TABLE `tz_typesize` MODIFY COLUMN `drum_volume` VARCHAR(255) COMMENT '转鼓容量';
ALTER TABLE `tz_typesize` MODIFY COLUMN `loading_capacity` VARCHAR(255) COMMENT '装料限量';
ALTER TABLE `tz_typesize` MODIFY COLUMN `max_speed` VARCHAR(255) COMMENT '最高转速';
ALTER TABLE `tz_typesize` MODIFY COLUMN `max_sep_factor` VARCHAR(255) COMMENT '最大分离因素';
ALTER TABLE `tz_typesize` MODIFY COLUMN `power` VARCHAR(255) COMMENT '功率';
ALTER TABLE `tz_typesize` MODIFY COLUMN `size` VARCHAR(255) COMMENT '尺寸';
ALTER TABLE `tz_typesize` MODIFY COLUMN `url` VARCHAR(255) COMMENT '数据来源';
```
