### 整理的过程记录

### 讲稿-初稿

主题：数智设计加载大语言模型 AI 的落地思考

#### P0 —— 开场

尊敬的万董，各位领导，大家上午好。我这边是针对主题 2 做个人的思考分享。GPT 我年就开始接触，那个时候就每个月花 20 美元充 Plus 账号，从 GPT3/3.5/4/4V，到最近一次 11.6 日的GPTs、GPT4-Turbo，从网页交互界面到调它的 API，基本天天在用。但从 11 月下旬开始，我一直处于极度亢奋的状态，因为我终于有条件在本地电脑自己部署AI 大语言模型了，从一个纯消费应用端的人转化为生产端的人，看到的东西完全不一样的。12月份，我先后成功部署了北京智普 60 亿参数的 AI 大模型，阿里的 720亿参数的通义千问大模型。

#### P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

我们先看下面这个录屏。

这个是我本地部署的720 亿参数的通义千问，我同时把之前舒伟杰做 HAZOP 培训的一个录音文字版作为外挂知识库，让它生成文字时优先结合这个知识库的信息。比如我问题 HAZOP 常规的步骤、风险定级怎么定，它回复的内容，都是结合培训内容进行提炼总结推理生成的。

我简单说下，以 GPT 为代表的AI大语言模型基本的一个原理。它的本质是预测，预测下一个词。就跟刚刚大家看到的，它是一个词接一个词出来的。它的背后是一个复杂的神经网络系统，通过学习海量的文本数据，调整神经网络里各个节点的权重参数。当你给它一个输入时，它基于这些权重参数，按概率的大小控制信息在这个神经网络里流动，从而生成连贯的、有意义的输出。

#### P1 —— 知道 AI 的强项和局限在哪里

想要用好它，你至少得知道它的强项和局限在哪。

目前生成式 AI 的强项有：1）很强的文本理解能力，它能理解上下文。2）提示词做好的可以生成高质量的文本和代码。3）强大的多语言能力。4）较强的推理能力。

它目前的局限有：1）幻觉问题，模型发散，它的底层原理决定了这个特性。但目前有些技术是可以给它做限制的，前面演示的外管知识库就是一种手段。2）第二个是上下文长度显示，上下文一旦太长生成质量下降的很快，这点后面技术上肯定会被解决。

#### P1 —— 化工设计行业 AI 专家

百年天正，如果将生成式 AI 应用到我们公司，一个看得到的落地点是做一个我们行业的 AI 专家，也就是天正版 Chat。

这个应用，上面这半边，除了大模型微调，其他的我已经试通了，整个应用做出来能够得到。基本的环节：首先把所有的文本型东西都数字化、向量化，这里有规范、设计说明专篇、校审核记录、培训等等。第二步是哪这些数据，结合大模型微调、检索增强式生成技术，在开源通用大模型的基础上调出一个我们行业的大模型。最后，拿这个天正版 Chat 去做培训、辅助设计去生成我们专篇类的成品。

#### P1 —— 数智设计的AI大模型基座

可我不仅仅要想做文本性的东西，我们成品大头是图纸。那么结合到数智设计这边来。这里有 2 个架构，一个是我们现阶段数智设计的实现路径，一个是我设想的基于大模型的实现路径。

我们现在是自上而下的实现路径。各个专业提炼自己的设计业务逻辑，我们团队的产品经理先把业务逻辑转化为供开发用的需求文档，接着开发出一个个数智设计成图模块，最后生成成品图纸图表。

而基于大语言模型的实现是自下而上的。拿成品图纸图表数字化向量化，然后用这些数据预语料、微调出我们天正的 AI 设计大模型。接着用封装的提示词直接生成成品图纸图表，这些成品图纸图表又可以数字化向量化再去训练微调模型，形成正向循环的飞轮。

#### P3 —— 结束语

最后，分享一句我看到很有感触的话：

一切皆可向量化。然后再升到千维，发现前所未有的发现。

以上是我的分享，谢谢大家。







P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

大语言模型（LLM）是一个复杂的神经网络系统，它的核心在于通过学习大量文本数据，掌握语言的规律和模式，并能够根据给定的输入生成连贯、有意义的输出。这就像一个智能的水管系统，其中有很多水龙头代表不同的词汇，而管道网络则代表着这些词汇之间的关系。

当你向大语言模型询问一个问题时，它会通过调整一系列阀门（权重参数）来控制信息在神经网络中的流动。这个过程就像是训练一群聪明的松鼠去调整每个阀门，使得正确的水流从对应的水龙头流出。虽然这个比喻可能有些抽象，但它有助于我们理解大语言模型如何处理和生成语言。

总的来说，大语言模型通过学习和掌握大量文本数据中词汇之间的关系和模式，能够根据给定的输入生成连贯、有意义的输出。

---

大语言模型（LLM）可以看作是一个非常复杂的管道网络，其中每个节点都对应一个单词。当你向模型输入一句话时，这些单词就会变成水从对应的水龙头流出。

然后，这个管道网络中的阀门（也就是模型的参数）会根据上下文信息进行调整，以便让正确的单词按照预期顺序流出。在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。

整个过程就像是一群聪明的松鼠在复杂的管道网络中穿梭，调整每个阀门以控制水流。这个比喻可能有点夸张，但它可以帮助我们理解 LLM 是如何通过大量参数来处理和生成语言的。

请注意，虽然这个比喻有助于理解 LLM 的基本工作原理，但实际的模型要复杂得多，使用的是矩阵乘法等数学函数来实现信息的传递和处理。

---

大语言模型（LLM）是一个复杂的计算机程序，它的工作方式有点像水管网络。你可以想象有许多水管连接在一起，每个水管都代表不同的单词。当你输入一句话时，这些单词就像水一样从对应的水管中流出。

然后，这个水管网络中的阀门会根据句子的上下文信息进行调整，以便让正确的单词按照预期顺序流出。在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。

整个过程就像是有一群聪明的松鼠在复杂的水管网络中穿梭，调整每个阀门以控制水流。这个比喻可能有点夸张，但它可以帮助我们理解 LLM 是如何通过大量参数来处理和生成语言的。

---

当我们谈论大语言模型（LLM）时，可以将其比喻为一个复杂的水管系统。这个系统中有很多水龙头，每个水龙头对应不同的单词。

当我们想要生成文本时，就像打开正确的水龙头以让水流出一样，我们需要确定要输出的下一个单词。但是，仅靠打开正确的水龙头还不够，因为多个水龙头可能会连接到同一根水管上，这意味着单个阀门控制着多个水龙头的水流。

因此，大语言模型实际上是一个复杂的网络，其中有很多水管和阀门。当我们输入文本时，这个网络会通过调整相应的阀门来确定应该从哪些水龙头流出水。这样，我们就可以生成有意义的句子了。

请注意，这种比喻并不是非常精确，因为实际的大语言模型是基于神经网络构建的，并且使用的是数学函数而不是物理阀门。但是这个类比可以帮助我们理解 LLM 是如何根据输入信息进行处理和输出文本的。

---

提炼总结（一版）：

以下是针对大语言模型本质原理的总结提炼：

大语言模型（LLM）是一个复杂的神经网络系统，它通过学习大量文本数据，掌握词汇之间的关系和模式，并能够根据给定的输入生成连贯、有意义的输出。当向大语言模型询问一个问题时，它会通过调整一系列阀门（权重参数），控制信息在神经网络中的流动，以便让正确的输出生成。整个过程可以类比为一个智能的水管系统，其中水龙头代表词汇，管道网络则代表着这些词汇之间的关系。

在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。虽然实际的大语言模型使用矩阵乘法等数学函数来实现信息的传递和处理，但水管系统的类比有助于我们理解其工作原理。

总的来说，大语言模型通过学习文本数据中的词汇关系和模式，能够根据输入生成连贯、有意义的输出。

自己提炼：

它的本质是预测，预测下一个词。就跟刚刚大家看到的，它是一个词接一个词出来的。它的背后是一个复杂的神经网络系统，通过学习海量的文本数据，调整神经网络里各个节点的权重参数。当你给它一个输入时，它基于这些权重参数，按概率的大小控制信息在这个神经网络里流动，从而生成连贯的、有意义的输出。



请结合下面的内容，解释大语言模型（LLM）的本质原理，最好用通俗易通的比喻来讲解。

原文：

LLMs 的一个关键创新在于它们不需要显式标记的数据。它们是通过尝试预测普通文本段落中的下一个单词来学习的。几乎所有的书面材料 —— 从维基百科页面到新闻文章，甚至计算机代码 —— 都适合用来训练这些模型。

例如，一个 LLM 可能会得到「I like my coffee with cream and」这样的输入，并应预测「sugar」作为下一个单词。一个新初始化的语言模型在这方面做得非常糟糕，因为其每个权重参数 —— 在 GPT-3 最强大版本中有 1750 亿个 —— 最初都是基本随机的数字。

但随着模型接触到更多示例 —— 数千亿词 —— 这些权重逐渐调整，从而做出越来越准确的预测。

这里有一个类比来说明这个过程。假设你要洗澡，想要水温恰到好处：不太热，也不太冷。你之前没用过这个水龙头，所以你随机转动旋钮并感受水温。如果水太热，你会向一个方向转；如果太冷，你会向另一个方向转。你离理想温度越近，调整的幅度就越小。

现在让我们对这个类比做一些修改。首先，想象不是一个水龙头，而是 50,257 个水龙头。每个水龙头对应一个不同的词，比如 the、cat 或 bank。你的目标是让水只从对应序列中下一个单词的水龙头流出。

其次，在这些水龙头后面，有一个错综复杂且相互连接的管道网，这些管道上也装有许多阀门。因此，如果水从错误的水龙头流出，你不只是调整水龙头上的旋钮。你会派出一群聪明的松鼠，沿着每条管道向后追踪，并沿途调整它们遇到的每一个阀门。

这个过程会变得复杂，因为同一管道通常会通向多个水龙头。因此，需要仔细考虑哪些阀门该拧紧，哪些该放松，以及调整的程度。

显然，如果过于字面地理解这个例子，它会变得荒谬。建造一个拥有 1750 亿个阀门的管道网络既不现实也无用。但得益于摩尔定律，计算机可以并且确实处理这种规模的运作。

本文讨论的所有 LLMs 部分 —— 前馈层中的神经元和在单词间移动上下文信息的注意力头 —— 都是通过一系列简单的数学函数（主要是矩阵乘法）实现的，其行为取决于可调节的权重参数。就像故事中的松鼠通过松紧阀门来控制水流一样，训练算法通过增加或减少语言模型的权重参数来控制信息在神经网络中的流动。

训练过程分为两个步骤。首先是「前向传递」，打开水龙头检查水是否从正确的水龙头流出。然后关闭水龙头进行「反向传递」，其中松鼠在每条管道上奔跑，调整阀门。在数字神经网络中，松鼠的角色由一种称为反向传播的算法扮演，该算法「向后穿越」网络，使用微积分来估计每个权重参数的调整幅度。

完成这个过程 —— 对一个示例进行前向传递，然后进行反向传递以提高网络在该示例上的表现 —— 需要数千亿次数学运算。训练像 GPT-3 这样大型的模型需要重复这个过程数十亿次 —— 针对训练数据的每个词各一次。OpenAI 估计，训练 GPT-3 需要超过 300 亿万次浮点计算 —— 这相当于几十个高端计算机芯片数月的工作量。



