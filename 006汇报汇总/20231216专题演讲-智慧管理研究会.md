你是一位同时具有多年工程设计经验和计算机 AI 大模型领域知识的资深专家，现在在中国国内一家工程设计院担任公司数字化转型部门的负责人。你现在准备做一场主题分享，主题是{工程设计公司如何结合 AI 大模型做数字化转型}，请给出一篇演讲的讲稿，要求800字左右。

你是一位具有多年工程设计经验，现在在中国国内一家工程设计院担任总经理。当今世界俄乌、巴以冲突热战肆虐，脱钩断链、贸易战争持续不断，百年未遇之大变局深刻进行；放眼国内，制造业产能过剩，高速发展的势头受阻，经济发展不确定性增加；工程设计行业也深受影响。
请你思考中国国内工程设计公司的发展方向和发展目标？越详细越好，一步步来


背景信息：

1、会议议题：

「方向」和「目标」思考：

导言：当今世界俄乌、巴以冲突热战肆虐，脱钩断链、贸易战争持续不断，百年未遇之大变局深刻进行；放眼国内，制造业产能过剩，高速发展的势头受阻，经济发展不确定性增加；工程设计行业也深受影响，我们如何思考发展方向和发展目标？

思路：把最近几个也万董经营会议上的讲话做为提炼素材试试。

化工设计与 AI 思考：

导言：以 ChatGPT4 为代表的 AI 应用热潮，让我们感受到 AI 扑面而来；短期内各行各业将在 AI 的加持下迎来深刻变革，深处其中的我们如何做好准备，迎接设计行业 AI 的发展，如何建设和应用好公司的「数智设计」，助力公司快速稳健发展。

2、会议议程：

俞洪明、余建锋针对会议议题做研讨会导入式发言。

与会人员阐述自己的观点。

名誉会长万军董事长点评。

本次主持轮值会长俞洪明总结。



### 讲稿-二版

主题：数智设计加载大语言模型 AI 的落地思考

#### P0 —— 开场

尊敬的万董，各位领导，大家上午好。我这边做主题2的思考分享。GPT 我年就开始接触，每个月花 20 美元 Plus 账号，从 GPT3/3.5/4/4V，到最近一次 11.6 日的GPTs、GPT4-Turbo，从网页交互界面到调它的 API，基本天天在用。但从 11 月下旬开始，我一直处于极度亢奋的状态，因为我终于有条件在本地电脑自己部署AI 大语言模型了，从一个纯消费应用端的人转化为生产端的人，看到的东西完全不一样的。

#### P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

我们先看下面这个录屏。这是最近，我在自己电脑成功部署了阿里开源的720亿参数的大语言模型，同义千问。GPT3的规模参数是1750亿。我同时把之前舒伟杰做 HAZOP 培训的一个录音文字版作为外挂知识库，让它生成文字时优先结合这个知识库的信息。比如我问题 HAZOP 常规的步骤、风险定级怎么定，它回复的内容，都是结合培训内容进行提炼总结推理生成的。

我简单说下，以 GPT 为代表的AI大语言模型基本的一个原理。它的本质是预测，预测下一个词。就跟刚刚大家看到的，它是一个词接一个词出来的。它的背后是一个复杂的神经网络系统，通过学习海量的文本数据，调整神经网络里各个节点的权重参数。当你给它一个输入时，它基于这些权重参数，控制这段信息在这个神经网络里流动，从而生成连贯的、有意义的输出。

#### P1 —— 知道 AI 的强项和局限在哪里

想要用好它，你至少得知道它的强项和局限在哪。

目前生成式 AI 的强项有：1）很强的文本理解能力，它能理解上下文。2）提示词做好的可以生成高质量的文本和代码。3）强大的多语言能力。4）较强的推理能力。

它目前的局限有：1）幻觉问题，模型发散，它的底层原理决定了这个特性。但目前有些技术是可以给它做限制的，前面演示的外管知识库就是一种手段。2）第二个是上下文长度限制，上下文一旦太长生成质量下降的很快，这点后面技术上肯定会被解决。

#### P1 —— 化工设计行业 AI 专家

百年天正，如果将生成式 AI 应用到我们公司，一个看得到的落地点是做一个我们行业的 AI 专家，也就是天正版 Chat。

这个应用，上面这半边，除了大模型微调，其他的我已经试通了，整个应用做出来能够得到。基本的环节：首先把所有的文本型东西都数字化、向量化，这里有规范、设计说明专篇、校审核记录、培训等等。第二步是拿这些数据，结合大模型微调、检索增强式生成技术，在开源通用大模型的基础上调出一个我们行业的大模型。最后，拿这个天正版 Chat 去做培训、辅助设计去生成我们专篇类的成品。

#### P1 —— 数智设计的AI大模型基座

可我不仅仅要想做文本性的东西，我们成品大头是图纸。那么结合到数智设计这边来。这里有 2 个架构，一个是我们现阶段数智设计的实现路径，一个是我设想的基于大模型的实现路径。

我们现在是自上而下的实现路径。各个专业提炼自己的设计业务逻辑，我们团队的产品经理先把业务逻辑转化为供开发用的需求文档，接着开发出一个个数智设计成图模块，最后生成成品图纸图表。

而基于大语言模型的实现是自下而上的。拿成品图纸图表数字化向量化，然后用这些数据预语料、微调出我们天正的 AI 设计大模型。接着用封装的提示词直接生成成品图纸图表，这些成品图纸图表又可以数字化向量化再去训练微调模型，形成正向循环的飞轮。这种自下而上的方式我很早之前就想过，但没资源无从下手，但今年AI技术的大爆发，全世界顶尖公司都把大模型开源了，那么我们可以站在巨人的肩膀做一些我们行业内的事情了。

#### P3 —— 结束语

最后，分享一句我看到很有感触的话：

一切皆可向量化。然后再升到千维，发现前所未有的发现。

以上是我的分享，谢谢大家。





### 讲稿-初稿

主题：数智设计加载大语言模型 AI 的落地思考

#### P0 —— 开场

尊敬的万董，各位领导，大家上午好。我这边是针对主题 2 做个人的思考分享。GPT 我年就开始接触，那个时候就每个月花 20 美元充 Plus 账号，从 GPT3/3.5/4/4V，到最近一次 11.6 日的GPTs、GPT4-Turbo，从网页交互界面到调它的 API，基本天天在用。但从 11 月下旬开始，我一直处于极度亢奋的状态，因为我终于有条件在本地电脑自己部署AI 大语言模型了，从一个纯消费应用端的人转化为生产端的人，看到的东西完全不一样的。12月份，我先后成功部署了北京智普 60 亿参数的 AI 大模型，阿里的 720亿参数的通义千问大模型。

#### P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

我们先看下面这个录屏。

这个是我本地部署的720 亿参数的通义千问，我同时把之前舒伟杰做 HAZOP 培训的一个录音文字版作为外挂知识库，让它生成文字时优先结合这个知识库的信息。比如我问题 HAZOP 常规的步骤、风险定级怎么定，它回复的内容，都是结合培训内容进行提炼总结推理生成的。

我简单说下，以 GPT 为代表的AI大语言模型基本的一个原理。它的本质是预测，预测下一个词。就跟刚刚大家看到的，它是一个词接一个词出来的。它的背后是一个复杂的神经网络系统，通过学习海量的文本数据，调整神经网络里各个节点的权重参数。当你给它一个输入时，它基于这些权重参数，按概率的大小控制信息在这个神经网络里流动，从而生成连贯的、有意义的输出。

#### P1 —— 知道 AI 的强项和局限在哪里

想要用好它，你至少得知道它的强项和局限在哪。

目前生成式 AI 的强项有：1）很强的文本理解能力，它能理解上下文。2）提示词做好的可以生成高质量的文本和代码。3）强大的多语言能力。4）较强的推理能力。

它目前的局限有：1）幻觉问题，模型发散，它的底层原理决定了这个特性。但目前有些技术是可以给它做限制的，前面演示的外管知识库就是一种手段。2）第二个是上下文长度显示，上下文一旦太长生成质量下降的很快，这点后面技术上肯定会被解决。

#### P1 —— 化工设计行业 AI 专家

百年天正，如果将生成式 AI 应用到我们公司，一个看得到的落地点是做一个我们行业的 AI 专家，也就是天正版 Chat。

这个应用，上面这半边，除了大模型微调，其他的我已经试通了，整个应用做出来能够得到。基本的环节：首先把所有的文本型东西都数字化、向量化，这里有规范、设计说明专篇、校审核记录、培训等等。第二步是哪这些数据，结合大模型微调、检索增强式生成技术，在开源通用大模型的基础上调出一个我们行业的大模型。最后，拿这个天正版 Chat 去做培训、辅助设计去生成我们专篇类的成品。

#### P1 —— 数智设计的AI大模型基座

可我不仅仅要想做文本性的东西，我们成品大头是图纸。那么结合到数智设计这边来。这里有 2 个架构，一个是我们现阶段数智设计的实现路径，一个是我设想的基于大模型的实现路径。

我们现在是自上而下的实现路径。各个专业提炼自己的设计业务逻辑，我们团队的产品经理先把业务逻辑转化为供开发用的需求文档，接着开发出一个个数智设计成图模块，最后生成成品图纸图表。

而基于大语言模型的实现是自下而上的。拿成品图纸图表数字化向量化，然后用这些数据预语料、微调出我们天正的 AI 设计大模型。接着用封装的提示词直接生成成品图纸图表，这些成品图纸图表又可以数字化向量化再去训练微调模型，形成正向循环的飞轮。

#### P3 —— 结束语

最后，分享一句我看到很有感触的话：

一切皆可向量化。然后再升到千维，发现前所未有的发现。

以上是我的分享，谢谢大家。

### 整理的过程记录

#### P0 —— 开场

1、亢奋的状态的表述。

英伟达在中国禁售 A100 的显卡，目前一张 A100（80G）的显卡 18-20W。

苹果的 M 芯片实现本地部署

2、搭建的里程碑。

#### P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

1、边放视频边讲底层原理。几百上千亿个水龙头的隐喻。

#### P1 —— 知道 AI 的强项和局限在哪里

知道 AI 的强项和局限在哪里

先讲完强项和弱项，再讲目前的应用点，按这个逻辑来。


大语言模型的强项在于：

1、很强的文本理解能力

2、提示词 Prompt 得当的话可以生成高质量的文本和代码

3、强大的多语言能力

4、拥有较强的推理能力

大语言模型一些明显局限：

1、幻觉问题，会胡说八道，所以它生成的结果需要人工二次校对确认

2、上下文长度限制，上下文超过其记忆后生成质量下降得快



即使是现在号称能 200K Tokens 上下文长度的 Claude，内容一长的话，生成质量下降得很快；

Qwen72B 的上下文是 32K，主流这个参数级别的开源大模型基本都是 32K 的最大上下文。

32K 最大上下文对应的中文字符数量取决于不同的文本编码模型和实现方式。通常情况下，一个中文字符通常被编码成一个 Unicode 字符，通常是 2 字节（16 位）。如果一个文本编码模型的最大上下文为 32K，那么它最多能够处理 32 * 1024 / 2 = 16,384 个中文字符。

但需要注意的是，不同的文本编码模型和实现方式可能会有一些变化，这只是一个大致的估计。如果您想要更具体的信息，您可以查阅相关文档或参考特定模型的技术规格。

4,096 个 token 对应多少中文

一个标准的中文汉字通常占用一个 token，但由于汉字有很多变体和特殊情况，有时会占用多个 token。一般来说，4,096 个 token 可以对应大约 2000 到 3000 个中文汉字。然而，这个数字仍然会因文本的内容和使用的中文分词工具而有所不同。如果您有特定的文本和分词工具，您可以使用它们来准确计算 token 数量。

gpt-4-1106-preview：128K 的最大上下文，上下文窗口提高到 128k（相当于 300 页书籍）。

补充：评测结果是，砍一半，实际能记住的大约是 64K 的上下窗口，换算下差不多是 3.2W 的字（应该是包含输入输出）。

3、直接生成高质量的原创文章还做不到，还是需要人和 AI 结合来完成，但极大放大人的效率。



#### P1 —— 化工设计行业 AI 专家



#### P1 —— 数智设计的AI大模型基座

从文本内的成品图纸切入。



#### P3 —— 结束语

一切皆可向量化。然后再升到千维，发现前所未有的发现。

势必加速公司的数字化建设。




#### P3 —— 年度计划的内容阐述



公司数智设计开发：

1、专业间协同模块的推广应用及优化。

从后台数据看，目前有部分数智设计模块不落地，应用很少或没有应用，特别是多专业协调的，比如仪表选型、低压配电模块。需要专门投资源用于推广运营，根据反馈打磨优化模块。有些模块应用必要时建议强制推广。另外，需要对一些散装的功能点，再做一些整合，形成大块的功能模块。比如将工艺流程的散装辅助功能点和二维配管的功能点独立打包，也为后续产品市场化做准备。

2、各专业设计作业线剩余模块的开发。

参考外管专业的全覆盖，目标多实现几个专业的全覆盖，比如工艺、仪表专业。按之前 8 月份跟您汇报各个专业专业线时确定的后续每个专业还需做哪些内容。

3、咨询/总包相关业务的模块开发。

现在数智设计团队开发为全公司服务，除了设计业务，咨询和总包业务都会有相关的数智设计需求。比如咨询那边得 HAZOP 分析。

数智设计生产数据应用于公司项目管理/质量管理：

数智设计应用于公司质量管理：以某个一键式出图的模块为例。

1、确保100%全部应用。

2、发挥数智设计校审前置的优点，只要业务逻辑、数据库、模块库没有问题，只需校审输入数据。每个模块为校审核配套对应的校审核功能，让其只需关注该模型需要校审的输入性数据。同时，模块优化时可以对一些输入数据做自动校验和约束。

只要模块 100%全覆盖应用、模块打磨优化完善。数智设计真正应用到工作场景中，数智设计模块产出的成品图纸很多质量问题可以解决。另外，跨专业协同的模块，也可以也可以解决部分存在的专业间协同问题。

数智设计应用于公司项目管理：只能列举一些数智设计能够自动抓取的生产数据场景，因对公司项目管理认知有限，需与项目管理部交流探讨结合点。

1、每次设计人员用该模块时，自动抓取该项目改单体今天的业务数据量。比如工艺流程模块，可以获取一个项目某个车间每天的设备、管道、仪表数量及其变化。

2、类似于统一规定、计算书，用数智设计做的直接到公司系统。公司项目管理系统也能实时自动获取这个信息。

基于开源模型在本地训练出一个行业大模型产品：

备注：我个人最看好，也最想做的。打通做出来可以做为前 3 项工作提供核心大模型。明年年初会在自己的个人 mac 上初步实践本地大模型产品。

3.15 日 GPT-4 发布后就一直在关注，天天在用，一路跟着经历了所有重要版本的更新节点。目前已实践过的：1）直接调用 GPT 和国内其他大模块的 API。2）本地跑参数少的、已经训练好的大模型，比如实现了音频到书面文稿的自动化整理，取代了之前用的讯飞语音。3）有能力研究 GitHub 上的开源模型项目。4）有能力研究国外 Hugging Face 上的大模型以及数据集。

大模型产品思路1：目前已有的文本型数据（能收集到的所有规范、校审记录、回访记录等等）作为训练素材，训练出一个专家系统。应用场景：1）专家系统解答问题。2）专篇模块里其他章节文本内容的生成。3）图纸（图片格式）输入，自动校审核系统。等等。

大模型产品思路2：成品图纸的反向数字化，获取每张图纸目录下包含哪些具体的实体数据（11 月初摸索出的具体实施路径，缺本地大模型）。入库时抓取 CAD 图纸里文字、块属性数据，全部提取为文本；Excel 提取为文本。通过大模型的少样品泛化能力，关联统一规定里的部分数据作为prompt提示词，调大模型清洗整理文本数据为半结构化，挂载到图纸目录。

大模型产品思路3：通过训练的大模型取代目前自上而下的数智设计开发模式（核心模型，目前还无具体实施路径，但肯定要用到产品思路 2 里的反向数字化，待研究摸索）。目前数智设计模块里的业务逻辑，都是靠业务方专家提炼出来的，交给开发团队开发。业务逻辑提炼对专业要求很高，而且逻辑有遗漏，开发效率慢。反向通过成品图纸抓取数据，提取图纸里得「模式」实现相同或更好的模块成图效果。

思路：先用 GPT 探索要做的大模型产品，等后面开源模块或国内大模块步伐跟上来之后，再切回跑本地的模型。首选 Meta（Facebook 研究部门）的LLaMA2，以及国内智普公司的大模型。






### 碎片记录

2023-12-12

知道 AI 的强项和局限在哪里

面对再强大的对手，如果知己知彼，自然能百战百胜。当我们了解了 AI 的强项和局限，就不用担心会被 AI 战胜，相反能让它为我们所用。

以大语言模型为例，我总结下来它的强项在于：

1、很强的文本理解能力；

2、Prompt 得当的话可以生成高质量的文本和代码；

3、强大的多语言能力；

4、有一些简单的推理能力。

但也有一些明显局限：

1、「幻觉」问题，也就是会胡说八道，所以它生成的结果需要人工二次校对确认；

2、上下文长度限制，即使是现在号称能 200K Tokens 上下文长度的 Claude，内容一长的话，生成质量下降得很快；

3、要写出高质量的原创文章还做不到，比如像我这篇文章，就无法借助 AI 的帮助来完成。

清楚了它的强项和局限，那么我不仅不用焦虑，还能让扬长避短。

2023-12-09

Gemini 介绍：我们的超级 AI 模型 [译]

2023 年 12 月 6 日

让 AI 更贴近每个人的生活

来自 Google 和 Alphabet 首席执行官 Sundar Pichai 的寄语：

每一次技术革新都是推动科学突破、加快人类前进的好机会，也是改善我们的生活的大好时机。我认为，我们现在见证的 AI 革命将是我们一生中最为深远的改变，这种影响甚至超过了手机和互联网的普及。AI 的潜力无限，它不仅能够改善我们的日常生活，还能在更高层面上创造出非凡的机会。AI 将引领创新与经济的新浪潮，并以前所未有的规模推动知识、学习、创造力和生产力的提升。

我最激动的是，有机会让 AI 成为全世界每个人的得力助手。



2023-12-06

1、内化/固化隐形知识，让其外显出来。

培训视频 => 文本语料（知识）

大模型 => 数字人（生成语音/生成视频）


2023-12-03

1、结合新员工导师代表的演讲，知识管理，再结合今天实现的 chatglm 关联外部知识库的案例。

另外，录一个 Chat 的对话视频放最后面。

2、讲原理的时候，用 1750 亿个水龙头那个例子。

3、应该将 AI 视为人类创造力的放大器，而不是替代品。

在一个名为《未来制造》的日常 AI 播客中，拥有超过二十年广告代理经验的资深创意专业人士 Brian Sykes 说，应该将 AI 视为人类创造力的放大器，而不是替代品。

他说，人类元素对于引导 AI 和塑造最终产品至关重要。

在对话中最让我印象深刻的是从演员到导演的比喻。Sykes 这样描述了这种向导演工作的上升转变：

我认为随着生成性 AI 变得更加普遍，创意专业人士将越来越多地承担导演的角色。在你的设计生涯初期，你想通过完全负责最终产品来留下自己的印记。但随着你的进步，你变得负有指导下属去创造满足客户需求的结果的责任。因此，生成性 AI 将允许创意专业人士即使在职业生涯早期也与之互动。他们仍然需要知识来获得他们想要的结果，就像使用任何工具一样。但越来越多的时候，我们将成为定义信息传递的导演，同时仍保留核心的人类元素。

新工具的引入使创意人员能够更少地关注低层次的执行，更多地关注高层次的愿景。作为一名导演，你关心的是整体叙事结构、场景间的流动、观众体验等。这个想法对我很有吸引力。

2023-12-02

阳志平 2023/11/25
活水智能# 万物皆向量
我们正处在一个历史性的机会窗口：一切皆可向量化。然后再升到千维，发现前所未有的发现。这是人类一个超级变革的机遇窗口。这里有无数可以做的文章。
现在几乎一切传统知识体系都可以被推翻重做。这里面有巨大无比的套利空间。传统知识的组织方式发生变化了。打个比方，在爱因斯坦之前，我们观察世界的最小颗粒度是原子这类，现在深入到基本粒子，夸克、光子、胶子等等这些。
同样，这是人类第一次，可以对一切呈现、流通的知识都处理为更底层的数字。然后通过模型训练，升维为千维矩阵。并且成本每个人都可以承受。现在在席卷一切传统知识。文本、音频、视频、3d 模型、药物分子式等等。
更逗的是，这个千维矩阵发生了什么是个黑箱，即使模型训练者也不知道是什么。
这就是非常有趣之处，一切并不垄断在某一两个公司里面。我们随便拿个以前的学科难题去这个千维矩阵做点事就是大突破。




阳志平 2023/12/02

大模型时代来临之后，我经常的感触是，有限的肉体，赶不上日新月异的时代变迁。有无数值得做，也值得深入做的想法。那么，在自己心有余而力不足的前提之下，就是鼓励与支持更多同学来做。2、3 人组成一个产品团队，然后深耕某条产品线。按照我统一的方法论去打。某种意义上，这是一种新的 YC 模式。不过不同的是，我们是更大的时间周期、更低试错成本的做法。当成本降到最低，那么做什么都是胜出了。

也许，18 个月、3 年、6 年、12 年后，整个生态又不一样了。

大模型这波为什么来得这么猛？本质上，在于三件事。

数据：大量开源数据被释放出来了。尤其是英文数据。这些数据在大模型时代之前其实也普遍存在了，然而，受限于知识产权法律，无法大规模释放出来。而大模型巧妙地绕过了知识产权法律规定，让知识产权法成了一个模糊的灰色地带。因此，大规模释放出来了。我也拿到了无数数据了。

模型: openai 非常有勇气地，拿 10 亿美金试错，给大家证明文本领域 transformer 这条路线是可以跑通的。

而图像领域的扩散模型也被证明，可以跑通的。于是，大家大幅照搬即可。

算力：目前其实在快速下降。18 个月后，训练模型达到临界值所需算力会越来越小。而开源社区涌现的各类框架，也在大幅降低训练成本、推理成本。

目前在席卷一切。什么领域，都可以大模型化，毕竟，思路不同了。



P1 —— Qwen72B 对 HAZOP 分析问答的视频录制

大语言模型（LLM）是一个复杂的神经网络系统，它的核心在于通过学习大量文本数据，掌握语言的规律和模式，并能够根据给定的输入生成连贯、有意义的输出。这就像一个智能的水管系统，其中有很多水龙头代表不同的词汇，而管道网络则代表着这些词汇之间的关系。

当你向大语言模型询问一个问题时，它会通过调整一系列阀门（权重参数）来控制信息在神经网络中的流动。这个过程就像是训练一群聪明的松鼠去调整每个阀门，使得正确的水流从对应的水龙头流出。虽然这个比喻可能有些抽象，但它有助于我们理解大语言模型如何处理和生成语言。

总的来说，大语言模型通过学习和掌握大量文本数据中词汇之间的关系和模式，能够根据给定的输入生成连贯、有意义的输出。

---

大语言模型（LLM）可以看作是一个非常复杂的管道网络，其中每个节点都对应一个单词。当你向模型输入一句话时，这些单词就会变成水从对应的水龙头流出。

然后，这个管道网络中的阀门（也就是模型的参数）会根据上下文信息进行调整，以便让正确的单词按照预期顺序流出。在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。

整个过程就像是一群聪明的松鼠在复杂的管道网络中穿梭，调整每个阀门以控制水流。这个比喻可能有点夸张，但它可以帮助我们理解 LLM 是如何通过大量参数来处理和生成语言的。

请注意，虽然这个比喻有助于理解 LLM 的基本工作原理，但实际的模型要复杂得多，使用的是矩阵乘法等数学函数来实现信息的传递和处理。

---

大语言模型（LLM）是一个复杂的计算机程序，它的工作方式有点像水管网络。你可以想象有许多水管连接在一起，每个水管都代表不同的单词。当你输入一句话时，这些单词就像水一样从对应的水管中流出。

然后，这个水管网络中的阀门会根据句子的上下文信息进行调整，以便让正确的单词按照预期顺序流出。在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。

整个过程就像是有一群聪明的松鼠在复杂的水管网络中穿梭，调整每个阀门以控制水流。这个比喻可能有点夸张，但它可以帮助我们理解 LLM 是如何通过大量参数来处理和生成语言的。

---

当我们谈论大语言模型（LLM）时，可以将其比喻为一个复杂的水管系统。这个系统中有很多水龙头，每个水龙头对应不同的单词。

当我们想要生成文本时，就像打开正确的水龙头以让水流出一样，我们需要确定要输出的下一个单词。但是，仅靠打开正确的水龙头还不够，因为多个水龙头可能会连接到同一根水管上，这意味着单个阀门控制着多个水龙头的水流。

因此，大语言模型实际上是一个复杂的网络，其中有很多水管和阀门。当我们输入文本时，这个网络会通过调整相应的阀门来确定应该从哪些水龙头流出水。这样，我们就可以生成有意义的句子了。

请注意，这种比喻并不是非常精确，因为实际的大语言模型是基于神经网络构建的，并且使用的是数学函数而不是物理阀门。但是这个类比可以帮助我们理解 LLM 是如何根据输入信息进行处理和输出文本的。

---

提炼总结（一版）：

以下是针对大语言模型本质原理的总结提炼：

大语言模型（LLM）是一个复杂的神经网络系统，它通过学习大量文本数据，掌握词汇之间的关系和模式，并能够根据给定的输入生成连贯、有意义的输出。当向大语言模型询问一个问题时，它会通过调整一系列阀门（权重参数），控制信息在神经网络中的流动，以便让正确的输出生成。整个过程可以类比为一个智能的水管系统，其中水龙头代表词汇，管道网络则代表着这些词汇之间的关系。

在这个过程中，每一层都会添加新的上下文信息，并传递给下一层，最终形成一个完整的句子。虽然实际的大语言模型使用矩阵乘法等数学函数来实现信息的传递和处理，但水管系统的类比有助于我们理解其工作原理。

总的来说，大语言模型通过学习文本数据中的词汇关系和模式，能够根据输入生成连贯、有意义的输出。

自己提炼：

它的本质是预测，预测下一个词。就跟刚刚大家看到的，它是一个词接一个词出来的。它的背后是一个复杂的神经网络系统，通过学习海量的文本数据，调整神经网络里各个节点的权重参数。当你给它一个输入时，它基于这些权重参数，按概率的大小控制信息在这个神经网络里流动，从而生成连贯的、有意义的输出。



请结合下面的内容，解释大语言模型（LLM）的本质原理，最好用通俗易通的比喻来讲解。

原文：

LLMs 的一个关键创新在于它们不需要显式标记的数据。它们是通过尝试预测普通文本段落中的下一个单词来学习的。几乎所有的书面材料 —— 从维基百科页面到新闻文章，甚至计算机代码 —— 都适合用来训练这些模型。

例如，一个 LLM 可能会得到「I like my coffee with cream and」这样的输入，并应预测「sugar」作为下一个单词。一个新初始化的语言模型在这方面做得非常糟糕，因为其每个权重参数 —— 在 GPT-3 最强大版本中有 1750 亿个 —— 最初都是基本随机的数字。

但随着模型接触到更多示例 —— 数千亿词 —— 这些权重逐渐调整，从而做出越来越准确的预测。

这里有一个类比来说明这个过程。假设你要洗澡，想要水温恰到好处：不太热，也不太冷。你之前没用过这个水龙头，所以你随机转动旋钮并感受水温。如果水太热，你会向一个方向转；如果太冷，你会向另一个方向转。你离理想温度越近，调整的幅度就越小。

现在让我们对这个类比做一些修改。首先，想象不是一个水龙头，而是 50,257 个水龙头。每个水龙头对应一个不同的词，比如 the、cat 或 bank。你的目标是让水只从对应序列中下一个单词的水龙头流出。

其次，在这些水龙头后面，有一个错综复杂且相互连接的管道网，这些管道上也装有许多阀门。因此，如果水从错误的水龙头流出，你不只是调整水龙头上的旋钮。你会派出一群聪明的松鼠，沿着每条管道向后追踪，并沿途调整它们遇到的每一个阀门。

这个过程会变得复杂，因为同一管道通常会通向多个水龙头。因此，需要仔细考虑哪些阀门该拧紧，哪些该放松，以及调整的程度。

显然，如果过于字面地理解这个例子，它会变得荒谬。建造一个拥有 1750 亿个阀门的管道网络既不现实也无用。但得益于摩尔定律，计算机可以并且确实处理这种规模的运作。

本文讨论的所有 LLMs 部分 —— 前馈层中的神经元和在单词间移动上下文信息的注意力头 —— 都是通过一系列简单的数学函数（主要是矩阵乘法）实现的，其行为取决于可调节的权重参数。就像故事中的松鼠通过松紧阀门来控制水流一样，训练算法通过增加或减少语言模型的权重参数来控制信息在神经网络中的流动。

训练过程分为两个步骤。首先是「前向传递」，打开水龙头检查水是否从正确的水龙头流出。然后关闭水龙头进行「反向传递」，其中松鼠在每条管道上奔跑，调整阀门。在数字神经网络中，松鼠的角色由一种称为反向传播的算法扮演，该算法「向后穿越」网络，使用微积分来估计每个权重参数的调整幅度。

完成这个过程 —— 对一个示例进行前向传递，然后进行反向传递以提高网络在该示例上的表现 —— 需要数千亿次数学运算。训练像 GPT-3 这样大型的模型需要重复这个过程数十亿次 —— 针对训练数据的每个词各一次。OpenAI 估计，训练 GPT-3 需要超过 300 亿万次浮点计算 —— 这相当于几十个高端计算机芯片数月的工作量。



